## 6. 결정 트리 

### 6.2 예측 (1)
```
- 노드의 속성
1. sample 속성: 얼마나 많은 훈련 샘플이 적용되었는지 계산
2. value 속성: 노드에서 각 클래스에 얼마나 많은 훈련 샘플이 있는지 계산
3. gini 속성: 지니 불순도 계산
```
![image](https://github.com/simsoohyeon/Machine-Learning-Deep-Learning-Study/assets/127268889/dd3cbbec-cf17-4fdd-a400-55995a1ab89e)
```
지니 불순도란, 결정 트리에서 사용되는 노드의 불순도를 측정하는 지표
불순도는 노드가 얼마나 혼합되어있는지를 나타내며, 지니 불순도가 낮을수록 해당 노드는 더 '순수한' 것이다
순수한 노드는 대부분 하나의 클래스에 속한 샘플들로 이루어져 있다
```
#### 모델 해석: 화이트 박스와 블랙 박스
```
- 화이트 박스 모델: 결정 트리와 같이 직관적이고 결정 방식을 이해하기 쉬운 모델
- 블랙 박스 모델
: 랜덤 포레스트나 신경망 등의 알고리즘 모델, 성능이 뛰어나고 예측 만드는 연산 과정 쉽게 확인 가능
but 왜 그런 예측을 만드는지는 설명하기 어려움
- 결정 트리는 필요하다면 수동으로 직접 따라 해볼 수 있는 간단하고 명확한 분류 방법 사용
  - 해석 가능한 ML(interpretable ML) 분야는 사람이 이해할 수 있는 방식으로 모델의 결정 설명할 수 있는 ML 시스템 만드는 것을 목표
  - 이는 시스템이 불공정한 결정을 내리지 않도록 하는 많은 영역에서 중요
```
### 6.7 규제 매개변수
```
from sklearn.datasets import make_moons
-> sklearn.datasets 모듈에서 make_moons 함수 불러오기, 이 함수는 두 개의 반달 모양 데이터 포인트 생성하는데 사용

X_moons, y_moons = make_moons(n_samples=150, noise=0.2, random_state=42)
-> 함수 이용하여 150개의 샘플로 구성된 데이터셋 생성
noise=0.2 는 약간의 잡음을 추가하며, 난수 생성기의 시드를 설정하여 재현성을 보장함
X_moons는 특성으로 구성된 배열이고, y_moons는 타겟 레이블로 구성된 배열

tree_clf1 = DecisionTreeClassifier(random_state=42)
-> 첫 번째 결정트리 분류기 생성, 결과의 재현성 보장

tree_clf2 = DecisionTreeClassifier(min_samples_leaf=5, random_state=42)
-> 두 번째 결정 트리 분류기 생성,min_sample_left=5는 리프 노드에 최소 5개의 샘플이 있어야 한다는 규제 조건 추가

tree_clf1.fit(X_moons, y_moons)
-> 첫 번째 결정 트리 분류기를 X_moons와 y_moons 데이터에 맞춰 학습

tree_clf2.fit(X_moons, y_moons)
-> 두 번째 결정 트리 분류기를 X_moons와 y_moons 데이터에 맞춰 학습
```
![image](https://github.com/simsoohyeon/Machine-Learning-Deep-Learning-Study/assets/127268889/300b4c5c-b988-48aa-9a92-11a3544a51f7)
```
그림은 두 개의 결정 트리 모델의 결정 경계를 시각화한 것
1. 왼쪽 그림(규제 없음)
: 첫 번째 결정트리 모델의 경계로 규제를 사용하지 않아 데이터에 매우 잘 맞추려 하기 때문에
결정 경계가 매우 복잡하고 세부적인 패턴을 많이 따름, 데이터의 잡음과도 잘 맞추려 하기 때문에
과적합이 될 가능성이 높음 -> 새로운 데이터에 대해 일반화 성능 떨어짐
2. 오른쪽 그림(규제 있음)
: 두 번째 결정트리 모델의 경계로, 리프 노드에 최소 5개의 샘플이 있어야 하는 규제 사용
결정 경계가 더 단순하고 부드럽게 나타남, 이는 모델이 과적합을 피하고 더 잘 일반화할 수 있도록 도움
노드에 최소 샘플 수를 설정함으로써, 모델은 덜 복잡해지고 더 안정적인 예측 가능해져 일반화 성능 향상

결론: 결정 트리 모델에 규제를 추가함으로써 과적합을 줄이고 모델의 일반화 성능을 개선할 수 있음을 확인
```

```
다른 랜덤 시드로 생성한 테스트 세트에서 두 결정 트리를 평가
X_moons_test, y_moons_test = make_moons(n_samples=1000, noise=0.2, random_state=43
-> make_moons 함수 사용하여 테스트용 데이터셋을 생성, 이 데이터셋은 1000개의 샘플로 구성
약간의 잡음 추가, 난수 생성기 통해 결과의 재현성 보장, 특성과 타겟 레이블로 구성된 배열

tree_clf1.score(X_moons_test, y_moons_test)
-> 첫 번째 결정 트리 모델의 테스트 데이터셋에 대한 정확도를 평가함
score 메소드는 주어진 특성과 타겟 데이터에 대해 모델의 정확도 반환함

tree_clf2.score(X_moons_test, y_moons_test)
-> 두 번째 결정 트리 모델의 테스트 데이터셋에 대한 정확도 평가함
```

### 6.8 회귀 (1)
```
사이킷런의 DecisionTreeRegressor를 사용해 잡음이 섞인 2차 함수 형태의 데이터셋에서
max_depth=2 설정으로 회귀 트리 만들기

import numpy as np
from sklearn.tree import DecisionTreeRegressor
-> numpy 라이브러리를 np로 임포트 하고, sklearn.tree 모듈에서 클래스 임포트
DecisionTreeRegressor는 결정 트리를 사용한 회귀 모델을 만드는데 사용됨

np.random.seed(42)
-> 난수 생성기로 시드를 42로 설정하여 결과의 재현성 보장함
이렇게 하면 코드가 실행될 때마다 동일한 난수르 생성하게 됨

X_quad = np.random.rand(200, 1) - 0.5  # 랜덤한 하나의 입력 특성
-> 200개의 샘플을 가지고 하나의 특성으로 이루어진 랜덤한 입력 데이터를 생성함
값의 범위는 -0.5부터 0.5 사이임

y_quad = X_quad ** 2 + 0.025 * np.random.randn(200, 1)
-> 출력 값을 생성함, 입력값 제곱에 약간의 잡음을 더한 값임, 잡음의 평균은 0이며 표준편차가 0.025

tree_reg = DecisionTreeRegressor(max_depth=2, random_state=42)
-> 결정트리 회귀 모델 생성, 트리의 최대 깊이를 2로 제한함, 결과의 재현성 보장

tree_reg.fit(X_quad, y_quad)
-> 결정 트리 회귀 모델을 생성한 데이터 X_quad와 y_quad에 맞춰 학습 시킴
```
### 6.8 회귀 (2)
![image](https://github.com/simsoohyeon/Machine-Learning-Deep-Learning-Study/assets/127268889/088f8ae8-bb0d-4563-8e9e-40cb8476694f)
```
- 각 영역의 예측값은 항상 그 영역에 있는 타깃값의 평균값이 됨
- 알고리즘은 예측값과 가능한 한 많은 샘플이 있도록 영역을 분할

- max_depth=2
: 단순한 모델로, 데이터의 전반적인 패턴을 따르지만 복잡한 패턴을 놓칠 수 있음
과적합의 위험은 적지만 과소적합이 될 가능성이 있음
-max_depth=3
: 더 보잡한 모델로, 데이터의 세부적인 변동을 더 잘 캡처함
과적합의 위험은 있지만 적절한 규제를 통해 이를 방지할 수 있음

두 그래프 모두 결정 트리의 깊이에 따라 예측이 어떻게 변화하는지 보여준다
더 깊은 트리는 복잡한 패턴을 학습할 수 있지만 과적합의 위험도 증가시킨다
따라서 결정 트리 모델의 깊이는 데이터의 복잡성과 일반화 성능을 고려하여 적절히 선택해야 함
분류에서와 같이 회귀 작업에서도 결정 트리가 과대적합되기 쉬우므로 규제가 필요
```

### 6.9 축 방향에 대한 민감성 (1)
```
- 결정트리는 계단 모양의 결정 경계를 만듦, 모든 분할은 축에 수직 (데이터의 방향에 민감)
=> 훈련 세트의 회전에 따라 결정트리의 축이 변할 수 있음
```

### 6.9 축 방향에 대한 민감성 (2)
```
from sklearn.decomposition import PCA
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler
-> PCA 주성분 분석을 위한 클래스
-> make_pipeline 여러 변환기 transformer와 추정기 estimator를 연결하여 파이프라인 만드는 함수
-> StandardScaler 데이터를 표준화(평균을 0, 분산을 1)로 하는 변환기

pca_pipeline = make_pipeline(StandardScaler(), PCA())
-> StandardScaler와 PCA를 순차적으로 적용하는 파이프라인 생성
이 파이프라인은 먼저 데이터를 표준화한 후 주성분 분석을 수행함

X_iris_rotated = pca_pipeline.fit_transform(X_iris)
-> X_iris 데이터를 파이프라인에 맞춰 변환함, 데이터는 먼저 표준화한 후 주성분 분석을 통해 회전
X_iris_ratated는 변환된 데이터임

tree_clf_pca = DecisionTreeClassifier(max_depth=2, random_state=42)
-> 최대 깊이가 2로 제한된 결정 트리 분류기를 생성, 결과의 재현성 보장

tree_clf_pca.fit(X_iris_rotated, y_iris)
-> 변환된 데이터와 타겟값 y_iris를 사용해 결정 트리 분류기 학습시킴

<결과 해석>
- 다른 색과 모양의 점들은 각각 다른 붗꽃 품종을 나타낸다
- 결정 트리가 각 클래스를 분류하기 위해 학습한 경계를 시각화한 것이다 깊이가 2로 제한
- 깊이 0에서 처음 분할이 이루어지고, 깊이 1에서 두 번째 분할이 이루어짐
- PCA를 통해 데이터의 차원을 축소하고 회전함으로써 결정트리가 각 클래스를 분류하는데 주요 특성을 쉽게 식벼
- 이 결과는 결정 트리 모델이 PCA로 변환된 데이터에서 잘 작동할 수 있음을 시사
```

### 6.10 결정 트리의 분산 문제
```
결정 트리의 주요 문제는 분산이 상당히 큼
-> 즉, 하이퍼파라미터나 데이터를 조금만 변경해도 매우 다른 모델이 생성될 수 있음
```
