## 8. 차원 축소
### 8.1 차원의 저주
```
- 차원의 저주
: 훈련 샘플 각각이 수천 심지어 수백만 개의 특성을 가지고 있어서 많은 특성은 훈련을 느리게 하고
좋은 솔루션을 찾기 어려움
- 3차원 세계에서 고차원 공간은 직관적 상상 불가, 차원이 클수록 과대적합의 위험 커짐
```
### 8.2 차원 축소를 위한 접근법 (1,2,3)
```
- 투영: 모든 훈련 샘플이 고차원 공간 안의 저차원 부분 공간에 놓여 있음
- 데이터셋의 차원을 3D에서 2D로 축소
- 스위스롤 Swiss roll 데이터셋: 부분 공간이 뒤틀리거나 휘어있음
```
### 8.2 차원 축소를 위한 접근법 (4)
![image](https://github.com/simsoohyeon/Machine-Learning-Deep-Learning-Study/assets/127268889/a7ab305d-35fb-4be7-8c74-3dfe2653a08b)
```
- 왼쪽 그래프, 원본 데이터 (평면에 그냥 투영시켜서 뭉개진 것)
: 이 그래프는 원본 데이터가 어떻게 분포되어 있는지 보여줌
x1 x2 라는 두 개의 특성을 사용하여 데이터를 시각화
데이터 포인트가 겹치고 각 클래스가 구별되지 않게 혼재되어 있음
=> 단순한 투영으로는 데이터의 구분이 어렵다는 것을 의미

- 오른쪽 그래프, 스위스 롤 변환 적용 후 (스위스 롤을 펼쳐 놓은 것)
: 이 그래프는 데이터에 비선형 변환, 스위스 롤 변환 적용한 후의 분포
z1 z2 라는 새로운 축을 사용하여 데이터를 시각화
비선형 변환을 통해 데이터의 구조가 펼쳐져 서로 다른 클래스나 그룹이 더 잘 구분되도록 변환
데이터 포인트들이 더 명확하게 구분되어 각 클래스가 더 잘 드러남 
```

### 8.2 차원 축소를 위한 접근법 (5)
```
- 매니폴드 학습: d차원 매니폴드
국부적으로 d차원 초평면으로 보일 수 있는 n차원 공간의 일부
스위스 롤의 경우에는 d=2, n=3

- 매니폴드 가정 또는 매니폴드 가정
대부분 실제 고차원 데이터셋이 더 낮은 저차원 매니폴드에 가깝에 놓여있다는 가설
```
### 8.2 차원 축소를 위한 접근법 (6)
![image](https://github.com/simsoohyeon/Machine-Learning-Deep-Learning-Study/assets/127268889/f77f3754-9c87-4b16-a2b5-71ced4173175)
```
- 매니폴드 가정 설명
: 데이터가 고차원 공간에 있을 때 실제로는 더 낮은 차원의 매니폴드에 놓여있다고 가정
이러한 가정을 통해 데이터의 구조를 잘 이해하고 적절한 차원 축소 기법 적용
하지만 매니폴드 가정이 항상 간단하거나 유효하지 않을 수 있음

- 3D 시각화
: 왼쪽 위와 아래의 두 그래프는 3차원 공간에서의 데이터 분포 시각화
데이터 포인트는 서로 다른 색으로 표시되어, 두 개의 클래스나 그룹을 나타냄
이런 3차원 구조는 복잡한 매니폴드 형태를 가짐

- 2D 시각화
: 오른쪽 위와 아래의 두 그래프는 2차원 공간으로 차원을 축소한 후의 데이터 분포 시각화
차원 축소 기법을 통해 3->2차원으로 표현했지만 데이터의 구조적 복잡성은 여전히 유지
이는 차원 축소 후에도 데이터의 분포가 8.3 여전히 복잡하며, 간단하게 구분되지 않음

- 결정 경계의 복잡성
: 차원 축소 후에도 결정 경계가 여전히 간단하지 않음
이는 데이터의 본질적인 복잡성이 차원 축소에 의해 단순화되지 않음을 나타냄
고차원에서 복잡한 데이터 구조는 저차원에서도 복잡할 수 있으며,
이는 적절한 모델 선택과 복잡한 경계 처리를 필요로 함
데이터의 본질적인 특성을 반영한 모델링 접근이 필요
```

### 8.3 주성분 분석(2)
```
- 주성분: PCA는 훈련 세트에서 분산이 최대인 축을 찾아감
- 고차원 데이터셋이라면 데이터셋에 있는 차원의 수만큼 n번째 축을 찾음
```

### 8.3 주성분 분석 (3)
```
넘파이의 svd() 함수를 사용해 3D 훈련 세트의 모든 주성분을 구한 후 처음 두 개의
PC를 정의하는 두 개의 단위 벡터를 추출

import numpy as np
-> numpy 파이썬에서 수치 계산을 위한 강력한 라이브러리로 특히 행렬 연산에 유용

X = [...]  # 작은 3D 데이터셋을 만듭니다.
-> X 변수에 3차원 데이터셋 할당, 3차원 공간에 있는 여러 데이터 포인트 나타냄

X_centered = X - X.mean(axis=0)
-> X.mean(axis=0) X의 각 축을 따라 평균을 계산, 변수에 중심화된 데이터 셋 저장
원본 데이터 X에서 각 축의 평균을 빼서 데이터셋을 중심화,
중심화는 데이터의 평균이 원점에 위치하도록 만드는 과정

U, s, Vt = np.linalg.svd(X_centered)
-> np.linalg.svd 함수 사용하여 중심화된 데이터셋에 대해 특이값 분해 SVD 수행
SVD 는 행렬을 세 개의 다른 행렬 U,s,Vt로 분해
U : 좌측 특이 백터 행령
s: 특이값 벡터
Vt: 전치된 우측 특이 벡터 행렬

c1 = Vt[0]
c2 = Vt[1]
-> Vt 행렬의 첫 번째 행을 c1,c2 변수에 저장
이는 중심화된 데이터셋의 첫 번째 주성분, 두 번째 주성분
```
### 8.3 주성분 분석 (5,6)
```
PCA 모델 사용해 데이터셋의 차원을 2로 줄이는 코

from sklearn.decomposition import PCA
-> 모듈에서 PCA 클래스 불러옴, 데이터의 차원을 축소하는데 사용되는 기법

pca = PCA(n_components=2)
-> PCA 클래스의 인스턴스 생성
n_components=2 주성분 분석을 통해 데이터의 차원을 2차원으로 축소 

X2D = pca.fit_transform(X)
-> pca 객체를 사용해 데이터 X에 대해 주성분 분석을 수행하고,
변환된 데이터를 X2D에 저장
```
```
3D 데이터셋의 처음 두 주성분에 대한 설명된 분산의 비율

pca.explained_variance_ratio_
-> PCA 변환을 수행한 후 각 주성분이 전체 데이터 분산에서 설명하는 비율 나타내는 속성
이 속성은 주성분들이 데이터의 변동성을 얼마나 잘 설명하는지 보여줌
array([0.7578477 , 0.15186921])
출력 해석: 처음 두 주성분이 전체 데이터 분산의 약 90% 설명
3차원 데이터셋에서 처음 두 주성분만 사용해도 데이터의 변동성 대부분을 설명할 수 있음
주성분 분석을 통해 데이터 차원을 축소하더라도, 데이터의 주요 특성을 대부분 유지 가능
```

### 8.3 주성분 분석(7)
```
MNIST 데이터셋을 로드하고 분할한 다음 차원을 줄이지 않고 PCA 수행

from sklearn.datasets import fetch_openml
-> 모듈에서 fetch_openml 함수 불러옴, 이 함수는 OpenML에서 데이터셋 로드

mnist = fetch_openml('mnist_784', as_frame=False)
-> MNIST 데이터셋의 ID, 데이터셋을 DF가 아닌 numpy 배열로 로드해 변수에 저장

X_train, y_train = mnist.data[:60000], mnist.target[:60000]
-> 데이터셋의 첫 6만개 샘플을 훈련 데이터로 사용 

X_test, y_test = mnist.data[60000:], mnist.target[60000:]
-> 데이터셋의 나머지 샘플을 테스트 데이터로 사용

pca = PCA()
-> 클래스의 인스턴스 생성, PCA 객체 만들기

pca.fit(X_train)
-> PCA 객체 사용하여 훈련 데이터에 대해 주성분 분석 수행
fit 메소드는 데이터의 주성분을 습

cumsum = np.cumsum(pca.explained_variance_ratio_)
-> pca.explained_variance_ratio_ 속성을 사용하여
각 주성분이 설명하는 분산의 비율을 누적 합으로 계산

d = np.argmax(cumsum >= 0.95) + 1  # d = 154
-> np.argmax 첫 번재 True값의 인덱스 반환해, +1 주성분의 개수를 구하기 위해 인덱스 값 증가
주성분의 누적 분산 비율을 계산하여 데이터의 95% 분산을 설명하는데
필요한 주성분의 개수를 찾음, 여기서는 154개의 주성분 필요 
```

### 8.3 주성분 분석(8)
```
n_components=d로 설정하여 PCA를 다시 실행

pca = PCA(n_components=0.95)
-> n_components=0.95 데이터의 95% 분산을 설명하는데 필요한 최소 개수의 주성분 자동 선택
이 설정은 훈련 데이터에서 95% 분산을 설명하는데 필요한 주성분의 개수를 자동으로 결정

X_reduced = pca.fit_transform(X_train)
-> pca 객체 사용해 훈련에 대해 주성분 분석을 수행하고 변환된 데이터 저장
원본 데이터의 95% 분산을 설명하는 주성분들로 구
```
```
실제 주성분 개수는 훈련 중에 결정되며 n_components_ 속성에 저장

pca.n_components_
-> pca가 설명한 실제 주성분 개수 확인, 여기에선 154개의 주성분 선택됨
```

### 8.3 주성분 분석 (9)
![image](https://github.com/simsoohyeon/Machine-Learning-Deep-Learning-Study/assets/127268889/b1ee7693-4798-482d-b7a4-42ec1802cb1f)
```
이미지는 주성분 분석에서 설명된 분산을 차원 수에 대한 함수로 시각화한 그래프
이를 통해 데이터의 분산을 설명하는데 필요한 주성분의 개수를 이해할 수 있음

X축: 차원 수(주성분의 개수), Y축: 설명된 분산 비율(누적 분산 비율)

왼쪽 하단에서 시작하여 오른쪽 상단으로 갈수록 점차 평평해지는 형태로
처음 몇 개의 주성분이 대부분의 분산을 설명한다는 것을 의미

엘보 Elbow: 그래프에서 급격히 변하는 지점, 주성분의 수 선택할 때 중요한 기준
엘보 지점 이후로는 주성분의 추가가 설명된 분산 비율의 증가에 큰 영향 미치지 않음
엘보 지점과 95%의 설명된 분산 비율을 사용하여 주성분의 적절한 수 선택 가능
이를 통해 데이터의 차원을 효과적으로 축소하면서도 대부분의 정보를 유지할 수 있음
```
### 8.3 주성분 분석 (10)
```
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import RandomizedSearchCV
from sklearn.pipeline import make_pipeline
-> 랜덤 포레스트 분류기를 사용하기 위해 모듈에서 불러옴
-> 하이퍼 파라미터 튜닝을 위한 랜덤 검색 교차 검증 기능을 사용하기 위해 모듈에서 불러옴
-> 파이프라인 쉽게 만들기 위해 모듈에서 불러옴

clf = make_pipeline(PCA(random_state=42), RandomForestClassifier(random_state=42))
-> make_pipeline 함수를 사용하여 PCA와 랜덤 포레스트 분류기를 연결한 파이프라인 객체 clf 만듦
PCA 주성분 분석을 사용하여 데이터 차원 축소
RandomForestClassifier 랜덤 포레스트 분류기 사용하여 분류 작업 수행

param_distrib = {
    "pca__n_components": np.arange(10, 80),
    "randomforestclassifier__n_estimators": np.arange(50, 500)
}
-> param_distrib: 랜덤 검색을 위한 하이퍼파라미터 분포를 정의하는 딕셔너리
PCA에서 사용할 주성분의 개수를 10에서 80까지의 범위에서 선택
랜덤 포레스트에서 사용할 결정 트리의 개수를 50에서 500까지의 범위에서 선택

rnd_search = RandomizedSearchCV(clf, param_distrib, n_iter=10, cv=3, random_state=42)
-> RandomizedSearchCV 객체 생성, clf: 하이퍼파라미터 튜닝을 수행할 파이프라인 모델
param_distrib: 하이퍼 파라미터 분포, n_iter=10: 하이퍼파라미터 조합 10번 시도
cv=3 : 3-폴드 교차 검증 사용

rnd_search.fit(X_train[:1000], y_train[:1000])
-> rnd_search 객체를 사용하여 랜덤 검색 교차 검증을 수행하고, 최적의 하이퍼파라미터 조합을 찾음
훈련 데이터의 첫 1000개 샘플 사용, 첫 1000개 샘플에 대한 레이블 사용

print(rnd_search.best_params_)
-> 최적의 하이퍼파라미터 조합을 확인
출력결과 확인:
{'randomforestclassifier__n_estimators': 465, 'pca__n_components': 23}
랜덤포레스트에서 사용할 결정트리의 수, PCA에서 사용할 주성분의 수
```

### 8.3 주성분 분석 (11,12) / 압축을 위한 PCA
```
- 공간 효율성: 차원 축소 후 훈련세트는 훨씬 더 적은 공간을 차지
데이터셋의 크기는 원본의 20% 이하로 줄지만 분산은 5%만 손실됨

- PCA의 복원 가능성: 압축된 데이터셋에 PCA의 투영의 반환을 반대로 적용하여 차원을 되돌림
투영에서 일정량의 정보를 잃어버렸기 때문에 이렇게 해도 원본 데이터셋을 얻을 수는 없음

- 재구성 오차: 원본 데이터와 재구성된 데이터(압축 후 원상 복구한 것) 사이의 평균 제곱 거리
- inverse_transform() 메소드: MNIST 데이터 집합을 다시 원래 차원 784개 차원으로 복원할 수 있음
```
```
X_recovered = pca.inverse_transform(X_reduced)
```
### 8.3 주성분 분석 (13)
```
- 랜덤 PCA 확률적 알고리즘
: svd_solver 매기변수를 이용하면 randomized로 지정하면 사이킷런은 처음 d개에 대한 근사값을 빠르게 찾음
svd_solver의 기본값인 auto
```
```
rnd_pca = PCA(n_components=154, svd_solver="randomized", random_state=42)

PCA 객체를 생성할 때 n_components=154로 지정하여 154개의 주성분을 선택
svd_solver=randomized로 설정하여 랜덤화된 svd 방식 사용, 이 방식은 빠른 근사값 찾음
random_state=42는 랜덤 초기화를 위한 시드값을 설정하여 결과의 재현성 보장

X_reduced = rnd_pca.fit_transform(X_train)

fit_transform(X_train) 메소드 호출하여 PCA 변환 학습,
훈련 데이터를 변환하여 축소된 데이터셋 얻음
```

### 8.3 주성분 분석 (14)
```
- 점진적 PCA (Incremental PCA, IPCA)
: 훈련 세트를 미니배치로 나눈뒤 IPCA 알고리즘에 한 번에 하나씩 주입
훈련 세트가 클 때 유용하며 온라인으로(새로운 데이터가 준비대는 대로 실시간) PCA 적용 가능
```
```
이 코드는 MNIST 훈련세트를 넘파이의 array_split() 함수를 사용해 100개의 미니배치로
나누고 사이킷런의 incrementalPCA 파이썬 클래스를 이용하여 MNIST 데이터셋의 차원을
이전과 같은 154개로 줄이는 코드임

from sklearn.decomposition import IncrementalPCA
# IncrementalPCA 모듈을 임포트

n_batches = 100
# 미니배치의 개수를 100으로 설정

inc_pca = IncrementalPCA(n_components=154)
# 154개의 주성분을 선택하여 IncrementalPCA 객체를 생성

for X_batch in np.array_split(X_train, n_batches):
# 훈련 데이터를 100개의 미니배치로 나눔
    inc_pca.partial_fit(X_batch)
# 각 미니배치에 대해 IncrementalPCA의 partial_fit 메서드를 호출하여 모델을 점진적으로 학습

X_reduced = inc_pca.transform(X_train)  # 전체 훈련 데이터셋을 변환하여 축소된 데이터셋을 얻습니다.

```
