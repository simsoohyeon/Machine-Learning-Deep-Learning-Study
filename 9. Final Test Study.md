## 9. 비지도학습
### 비지도학습 알고리즘
```
- 군집: 비슷한 샘플을 클러스터로 모음
- 이상치 탐지: 정상 데이터가 어떻게 보이는지 학습하고 비정상 샘플 감지
- 밀도 추정: 데이터셋 생성 확률 과정의 확률 밀도 함수 추정
밀도 추정은 데이터셋이 특정 공간에서 얼마나 밀집되어 있는지 추정하는 작업
확률 밀도 함수를 이추정하여 데이터가 특정 구간에서 속할 확률 계산
```

### 9.1 군집(1)
```
- 군집은 다양한 애플리케이션에서 사용
=> 고객분류, 데이터분석, 차원 축소 기법, 특성 공학, 이상치 탐지, 이미지 분할 등등
```
![image](https://github.com/simsoohyeon/Machine-Learning-Deep-Learning-Study/assets/127268889/547b0596-ef7d-48d6-8463-1730bea9a05e)

### 9.1 군집 (2)
```
- k-평균은 반복 몇 번으로 데이터셋을 빠르고 효율적으로 클러스터로 묶을 수 있는 간단한 알고리즘
비지도 학습 알고리즘으로 '로이드-포지 알고리즘' 이라고 불림
```
![image](https://github.com/simsoohyeon/Machine-Learning-Deep-Learning-Study/assets/127268889/2cf5cfed-941f-4999-b4de-83cc14caab97)

### 9.1 군집 (3)
```
- k-평균 알고리즘 훈련
각 클러스터의 중심점을 찾고 가장 가까운 클러스터에 샘플을 할당하는 과정

from sklearn.cluster import KMeans
from sklearn.datasets import make_blobs
# make_blobs는 인위적으로 클러스터 데이터셋을 할당

X, y = make_blobs([...])  # y는 클러스터 ID를 포함합니다.
# 하지만 이를 사용하지 않고 예측해보겠습니다.


k = 5 # 클러스터의 개수를 5라고 설정
kmeans = KMeans(n_clusters=k, random_state=42)
# KMeans의 클래스 인스턴스 생성, n_clusters 파라미터는 찾을 클러스터의 개수 설정
random_state는 결과의 재현성을 위해 설정

y_pred = kmeans.fit_predict(X)
# fit_predict 메소드 설정하여 x 데이터셋에 k-평균 알고리즘 적용
이는 데이터를 클러스터링 하고 예측된 클러스터 레이블 반

# KMeans 클래스의 인스턴스는 labels_ 인스턴스 변수를 가지고 훈련된 샘플의 예측 레이블을 가짐

>>> y_pred
array([4, 0, 1, ..., 2, 1, 0], dtype=int32)
# 예측된 클러스터 레이블이 저장된 배열 y_pred 출력, 각 값은 데이터 포인트가 속한 클러스터

>>> y_pred is kmeans.labels_
True
# 동일한 객체인지 확인, 동일함

```

### 9.1 군집 (4)
```
<K-평균 클러스터링, 센트로이드 및 예측>
- k-평균 알고리즘은 각 클러스터의 중심, 센트로이드를 찾음
- 새로운 샘플은 가장 가까운 센트로이드의 클러스터에 할당 가능함

>>> kmeans.cluster_centers_
array([[-2.8036916,  1.80117999],
       [ 0.20876306,  2.25551336],
       [-2.79209307,  2.79641063],
       [-1.46679593,  2.28585348],
       [-2.80037462,  1.30082566]])
# k평균 알고리즘이 찾은 클러스트의 중심, 출력된 값은 클러스터의 중심 좌표

import numpy as np
X_new = np.array([[0, 2], [3, 2], [-3, 3], [-3, 2.5]])
# 새로운 샘플 데이터를 X_new 배열로 정의함, 네 개의 샘플 포함

>>> kmeans.predict(X_new)
array([1, 1, 2, 2], dtype=int32)
# kmeans.predict 메소드 사용하여 새로운 샘플을 클러스터에 할당
각 샘플에 대해 가장 가까운 클러스트 중심을 찾아 해당 클러스트의 레이블 반환

새로운 데이터포인트를 기존의 클러스터에 할당할 수 있음

```

### 9.1 (5)
```
- 클러스터의 결정 경계, 보로노이 다이어그램
: 각 포인트에 대해 가장 가까운 중심점까지의의 거리를 기준으로 공간을 분할한 다이어그램

- k-평균 알고리즘의 제한: 클러스터의 크기가 다른 경우 잘 작동하지 않음
샘플을 클러스트에 할당할 때 센트로이드까지의 거리를 고려하기 때문임

그림은 k평균 알고리즘을 사용한 클러스터링의 결과를 보로노이 다이어그램으로 시각화
x1 x2는 데이터의 두 개의 특징
x로 표시된 점들은 각 클러스터의 중심점, 센트로이드
k평균 클러스터링의 경계는 선형적이지 않고 보로노이 다이어그램의 형태로 나타냄
센트로이드의 위치와 클러스터의 크기에 따라 셀 크기와 모양이 달라짐

```
![image](https://github.com/simsoohyeon/Machine-Learning-Deep-Learning-Study/assets/127268889/651bc94a-b0f4-44b5-b5d5-c1adab081fc2)

### 9.1 (6)

```
- 하드 군집과 소프트 군집 개념 및 KMeans의 transform() 메소드

- 하드 군집: 각 샘플을 하나의 클러스터에만 할당, 샘플이 속할 클러스터를 명확하게 결정
- 소프트 군집: 클러스터마다 샘플에 점수를 부여, 클러스터 간의 경계가 불명확하게 유용
샘플이 여러 클래스에 속할 수 있으며, 각 클러스터에 속할 확률이나 거리 기반의 점수를 가


>>> kmeans.transform(X_new).round(2)
array([[12.81,  0.33,  2.9 ,  1.49,  2.89],
       [15.81,  2.8 ,  5.85,  4.48,  5.84],
       [ 1.21,  3.29,  0.29,  1.69,  1.71],
       [ 0.73,  3.22,  0.36,  1.55,  1.22]])
# kmeans.transfrom(X_new) 메소드는 새로운 샘플 X_new와 각 센트로이드 사이의 거리를 계산,
KMeans의 transform 메소드는 소프트 군집의 일환으로 샘플과 각 클러스터 센트로이드거리 계산
 
```

### 9.1 (7)
```
K-평균 알고리즘

- 센트로이드를 랜덤하게 설정
: 처음에 데이터셋에서 k개의 샘플을 랜덤하게 뽑아 그 위치를 초기 센트로이드로 설정

- 샘플에 레이블을 할당하고 센트로이드를 업데이트
: 각 샘플을 가장 가까운 센트로이드에 할당하고 해당 클러스터에 포함
센트로이드를 업데이트하여 각 클러스터의 중심을 새로 계산
이 과정을 센트로이드의 변화가 없을 때까지 반복
=> 최종적으로 데이터셋을 k개의 클러스터로 분할 

- 반복 과정
: 샘플에 레이블을 할당하고 센트로이드를 변화가 없을 때까지 계속 반복
이 알고리즘은 제한된 횟수 안에 수렴하는 것을 보장(일반적으로 횟수는 매우 적음)
샘플과 가장 가까운 센트로이드 사이의 평균 제곱 거리는 각 단계마다 내려갈 수만 있고,
음수가 될 수 없기 때문에 수렴이 보장됨

```
### 9.1 (8)
```
- 이 그림은 k평균 알고리즘의 과정을 단계별로 보여줌

1. 초기화 단계
왼쪽 상단: 초기 센트로이드를 랜덤하게 선택, 빨간색 마커는 초기 센트로이드
오른쪽 상단: 각 데이터 포인트를 가장 가까운 초기 센트로이드에 할당

2. 첫 번째 업데이트
왼쪽 중간: 센트로이드 업데이트, 할당된 데이터 포인트들의 평균 위치 계산해 새롭게 설정
오른쪽 중간: 업데이트된 센트로이드에 따라 다시 데이터 포인트 할당, 클러스터 경계 변화

3. 두 번째 업데이트
왼쪽 하단: 센트로이드 업데이트, 할당된 데이터 포인트들의 평균 위치 계산해 새롭게 설정
오른쪽 하단: 최종 센트로이드 위치에 따라 데이터 포인트 재할당, 클러스터 경계가 최종적으로 설정

단계별 과정
1. 초기 센트로이드 선택 -> 2. 샘플 할당 -> 3. 센트로이드 업데이 -> 4. 반복

```


![image](https://github.com/simsoohyeon/Machine-Learning-Deep-Learning-Study/assets/127268889/f21cc515-3fd9-477e-be94-92b2d810bea5)

### 9.1 (9)
```
- k평균 알고리즘의 수렴과 초기화의 영향
- 수렴 보장: 수렴을 보장하지만 수렴하는 해가 최적의 해가 아닐 수 있음
즉, 지역 최적해일 수 있음

- 초기화의 중요성
센트로이드의 초기화가 잘못되면 최적의 클러스터링 결과를 얻지 못할 수 있음
초기 센트로이드 선택이 알고리즘에 최종 결과에 큰 영향 미침

그림 설명
해결책 1 : 왼쪽은 잘못된 센트로이드 초기화로 인해 형성된 클러스터
센트로이드가 데이터의 분포를 잘 반영하지 못해 비정상적인 클러스터 경계를 형성
특정 클러스터가 과도하게 작거나 다른 클러스터와 겹침

해결책 2: 오른쪽 그림은 다른 랜덤한 초기화로 인해 형성된 클러스터
센트로이드가 데이터 분포를 보다 잘 반영하여 상대적으로 더 나은 클러스터 경계를 형성
클러스터가 더 균형 잡히고 각 클러스터의 데이터 포인트들이 더 명확하게 분리

- 초기화의 영향: k평균 알고리즘의 성능은 초기 센트로이드 선택에 크게 의존
동일한 데이터에 비해 다른 초기값 선택하면 서로 다른 클러스터링 결과

- 지역 최적해: 알고리즘은 종종 전역 최적해 대신 지역 최적해에 수렴
여러 번의 초기화와 반복 통해 더 나은 클러스터링 결과 찾음

- 개선 방법
초기화를 여러 번 시도하고 최적의 결과를 선택하는 방식 k-means++ 사용
더 안정적이고 일관된 클러스터링 결과 보장


```

![image](https://github.com/simsoohyeon/Machine-Learning-Deep-Learning-Study/assets/127268889/3d190355-635e-4a66-b47f-eb60e3b87728)


### 9.1 (10)

```
- 센트로이드 초기화: 초기 센트로이드 위치에 대해 매우 민감
초기화를 잘 선택하면 더 좋은 클러스터링 결과 얻음

- 랜덤 초기화 반복
k평균 알고리즘을 여러 번 실행하고 가장 좋은 솔루션 선택
n_init 매개변수는 반복 횟수를 조절

- 이너셔 Inertia
이너셔는 성능 지표로 각 샘플과 가장 가까운 센트로이드 사이의 제곱 거리를 합한 값
이너셔 값이 작을수록 클러스터링이 잘 된 것을 의미

- score 메소드: 이너셔의 음수 값 반환

1. 초기 센트로이드 설정 
good_init = np.array([[1, 3], [-3, 2], [3, 1], [-1, 2], [0, 2]])
# good_init 배열에 5개의 초기 센트로이드 좌표 설정

2. k평균 모델 설정 및 학습
kmeans = KMeans(n_clusters=5, init=good_init, n_init=1, random_state=42)
kmeans.fit(X)
# kMeans 객체 생성하고 초기 센트로이드를 good_init로,
n_init을 1로 설정하여 한 번만 초기화를 실행
fit 메소드를 사용해 데이터를 클러스터링 

3. 랜덤 초기화를 다르게 하여 여러 번 실행
kmeans = KMeans(n_clusters=5, n_init=10, random_state=42)
# 객체 생성하여 10으로 설정해 10번의 다른 랜덤 초기화 시도, 가장 좋은 클러스터링 결과 

4. 모델의 이너셔 확인
kmeans.inertia_
# 이너셔는 각 샘플과 가장 가까운 센트로이드 사이의 제곱 거리 합

5. score 메소드 사용
kmeans.score(X)
# 음수 값을 반환하는 이유는 최적화 문제에서 점수가 클수록 좋은 방향으로 정규화하기 위
```

### 9.1 (11)
```
- K-평균++ 초기화 알고리즘
센트로이드 초기 위치를 보다 효과적으로 선택하여 k-평균 알고리즘의 성능을 향상시키는 방법

1. 첫 번째 센트로이드 선택: 데이터 셋에서 랜덤하게 하나 택, 이걸 c1라고 함

2. 다음 센트로이드 선택
각 데이터 포인트 x와 이미 선택된 센트로이드 중 가장 가까운 센트로이드 사이의 거리를 계산, D(x)
이 거리를 기반으로 각 데이터 포인트를 새로운 센트로이드로 선택할 확률 계산
거리가 먼 데이터 포인트들일수록 새로운 거 선택할 확률이 높아짐
이렇게 하면 이미 선택된 센트로이드에서 멀리 떨어진 포인트를 새로운 센트로이드로 선택할 가능성이 높아짐
이 과정을 통해 새롱누 센트로이드 c2로 선택

3. 반복
위 단계 반복하여 k개의 센트로이드를 모두 선택, 멀리 떨어진 데이터를 새로운 센트로이트로 선택할 확률 높아짐
```

### 9.1 (12)
```
- K 평균 속도 개선과 미니배치 K 평균
- 미니배치 k 평균
전체 데이터셋을 사용하는 k평균 알고리즘 반복과정과는 달리 미니배치 k 평균은 각 반복마다
전체 데이터셋 대신 작은 배치, 미니배치 사용 -> 알고리즘 속도 크게 개선
미니배치를 사용해 센트로이드를 조금씩 이동

- 사이킷런 클래스
사이킷런 라이브러리에서 미니배치 k평균을 쉽게 사용할 수 있도록 MiniBatchKMeans 클래스 제공

from sklearn.cluster import MiniBatchKMeans # 임포트 

minibatch_kmeans = MiniBatchKMeans(n_clusters=5, random_state=42)
# 객체 생성, 클러스터 개수 5개
minibatch_kmeans.fit(X)


그림 설명
- 이너셔
왼쪽 그래프는 k평균과 미니배치 k평균의 이너셔 변화 보여줌
미니배치가 더 평균이 높지만 이는 전체 데이터셋을 사용하지 않아서임
미니배치 평균의 이너셔가 더 빠르게 감소, k평균은 전체 데이터셋 사용하므로 더 낮음

- 훈련시간
미니배치가 훨씬 더 빠르게 수렴, 더 작은 배치를 사용하므로
k 평균은 전체 데이터셋을 사용하므로 시간이 오래 걸림

미니배치 k평균은 전체 데이터셋을 반복하지 않고 작은 배치를 사용해 센트로이드 조금씩 이동시킴
이를 통해 알고리즘 속도 크게 개선함

```
![image](https://github.com/simsoohyeon/Machine-Learning-Deep-Learning-Study/assets/127268889/2660d791-eb7e-4f5c-991a-294782c8e833)

### 9.1 (13)
```
- 잘못된 클러스터 개수 선택의 결과
- 클러스터 개수 선택의 중요성
적절하지 않은 클러스터 개수는 데이터의 잘못된 분류 초래
개수가 너무 작으면 합쳐지고, 너무 많으면 여러 개로 나뉨

그림 설명
왼쪽: 서로 다른 분포의 데이터들이 동일한 클러스터로 분류되면서 클러스터링 결과 왜곡
오른쪽: 실제로는 하나의 클러스터로 분류되어야 할 데이터들이 여러 클러스터로 나뉘며 세분화

너무 작은 k: 서로 다른 클러스터가 합쳐지며 분류가 부정확
너무 큰 k: 합쳐져야 할 데이터들이 세분화 
```
![image](https://github.com/simsoohyeon/Machine-Learning-Deep-Learning-Study/assets/127268889/c8ddaac5-b057-4281-a70d-56bb4bdffdc5)



### 9.1 (14)
```
- 클러스터 개수 k의 함수로 그린 이너셔 그래프

이너셔: 클러스터 내의 데이터 포인트들이 그 클러스터의 중심으로부터 얼마나 가까운지 측정
클러스터 내의 모든 점들까지의 거리의 합이며, 값이 작을수록 센트로이드에 가깝게 위치함을 의미

엘보방법: 클러스터 개수를 결정하는 방법 중 하나로 
```
![image](https://github.com/simsoohyeon/Machine-Learning-Deep-Learning-Study/assets/127268889/34511360-8281-4cf9-8c38-943fe5611e2f)


### 9.1 (15)
```
- 최적의 클러스터 개수 찾기: 실루엣 점수 사용
- 실루엣 점수: 클러스터링 품질을 평가하는 지표
각 데이터포인트가 얼마나 잘 클러스터링 되었는지 측정 -1~1사이의 값
1에 가까울수록 잘 클러스터링된 것
0이면 데이터포인트가 두 클러스터 경계에 위치한 것
-1이면 데이터포인트가 잘못된 클러스터에 할당된 것

from sklearn.metrics import silhouette_score # 사이킷런에서 클러스터링 실루엣 점수 계산

silhouette_score(x, kmeans.labels_)
# 실루엣 점수 계산, 원본 데이터와 k평균 알고리즘으로 예측된 클러스터 레이블

```
![image](https://github.com/simsoohyeon/Machine-Learning-Deep-Learning-Study/assets/127268889/1364760a-fcdc-409e-8730-3ad982b57a71)


### 9.1 (17)
```
- k평균의 한계와 장점
장점: 속도가 빠름, 확정성 용이
k평균 알고리즘은 비교적 빠르게 클러스터링을 수행할 수 있음
대규모 데이터셋에서도 효과적으로 동작

단점:
최적의 솔루션이 아닐 수 있음: 지역 최적해에 수렴할 수 있어 여러 번 알고리즘을 실행
클러스터 개수를 지정해야 함: 사전에 클러스터 개수를 알고 있어야 함
다양한 형태의 클러스터에 적합하지 않음
: 클러스터의 크기나 밀집도가 서로 다르거나, 원형이 아닌 경우 제대로 작동하지 않을 수 있음

```

![image](https://github.com/simsoohyeon/Machine-Learning-Deep-Learning-Study/assets/127268889/63972c46-9f95-4fa8-a9ae-932b259a7221)


### 9.1 (18)
```
이미지 분할: 여러 개의 세그 먼트로 나누는 작업, 이미지를 분석하고 처리

분할 방식
1. 색상 분할: 동일한 색상을 가진 세그먼트에 할당, 많은 어플리케이션에서 색상 분할만으로 충분
ex 인공위성 사진을 분석해 한 지역의 전체 산림 면적이 얼마나 되는지 측정
2. 시맨틱 = 사물 분할: 동일한 종류의 물체에 속한 모든 픽셀을 같은 세그먼트에 할당
ex 자율 주행 자동차에서 보행자 이미지를 구성하는 모든 픽셀은 보행자 세그먼트에 할당
3. 인스턴스 분할: 개별 객체에 속한 모든 픽셀을 같은 세그먼트
ex 각 보행자는 다른 세그먼트가 
```

### 9.1 (19), 9.1 (20)
```
- k평균 사용하는 간단한 색상 분할

import PIL
image = np.asarray(PIL.Image.open(filepath))
image.shape

- 배열의 크기를 바꿔 긴 RGB색상 리스트를 만든 다음 k평균 사용하여 8개의 클러스터로 모음
X = image.reshape(-1, 3)
kmeans = KMeans(n_clusters=8, random_state=42).fit(X)
segmented_img = kmeans.cluster_centers_[kmeans.labels_]
segmented_img = segmented_img.reshape(image.shape)
# kmeans.cluster_centers_[kmeans.labels_]를 통해 각 픽셀이 속하는 클러스터의 중심 색 얻음

=> 이미지를 8개의 색상 클러스터로 단순화된 이미지로 변환

이 그림은 k평균 알고리즘 사용하여 이미지 분할을 수행하였을 때 클러스터 수가
이미지의 세부 사항과 색상의 복잡성에 어떻게 영향을 미치는지 시각적을 보여줌
클러스터 수가 많을수록 원본 이미지에 더 가깝게 유지되며
클러스터 수가 적을수록 이미지가 단순화됨
```
![image](https://github.com/simsoohyeon/Machine-Learning-Deep-Learning-Study/assets/127268889/5abd6a12-c12a-4192-a2cd-59ba27a0f23e)


### 9.1 (21)

```
- 군집을 사용한 준지도 학습
준지도 학습은 레이블이 없는 데이터가 많고, 레이블이 있는 데이터가 적을 때 사용되는 기법
숫자 0~9까지를 나타내는 8*8 흑백이미지로 구성된 간단한 숫자 데이터셋 사용

from sklearn.datasets import load_digits # 8*8 손글씨 숫자 이미지 제공

X_digits, y_digits = load_digits(return_X_y=True)
# load_digits 함수 호출해 로드, return_X_y=True는 X_digits와 y_digits 반환
X_train, y_train = X_digits[:1400], y_digits[:1400]
X_test, y_test = X_digits[1400:], y_digits[1400:]
# 데이터를 훈련 세트와 테스트 세트로 나눔, 처음 1400개 훈련 나머지는 테스트 세트

- 50개 샘플에 대한 레이블만 있다고 가정하고 로지스틱 회귀 모델 훈련
from sklearn.linear_model import LogisticRegression # 로지스틱 회귀 모델 사용
n_labeled = 50 # 레이블 샘플 수를 50개로
log_reg = LogisticRegression(max_iter=10000) # 최대 만번의 반복 허용하는 객체 생성
log_reg.fit(X_train[:n_labeled], y_train[:n_labeled]) # 처음 50개 샘플을 사용해 모델 학습

손글씨 숫자 데이터 셋을 로드해 일부 샘플에 대해서만 레이블을 부여해
로지스틱 회귀 모델을 학습시키는 과정을 보여줌
준지도 학습 방법을 사용해, 레이블이 적은 상황에서도 모델 학

```

### 9.1 (22)
```
>>> log_reg.score(X_test, y_test)
0.7481108312342569 # 정확도 측정

from sklearn.cluster import KMeans
k = 50 # 클러스터 수 50개
kmeans = KMeans(n_clusters=k, random_state=42)
# k개의 클러스터인 객체 생성 
X_digits_dist = kmeans.fit_transform(X_train)
# 훈련 세트 X_train 사용해 k평균 알고리즘 실행
각 샘플과 클러스터 센트로이드 간의 거리를 계산해 변수에 저장
representative_digit_idx = np.argmin(X_digits_dist, axis=0)
# 각 클러스터 센트로이드에 가장 가까운 샘플의 인덱스를 계산해 저장
X_representative_digits = X_train[representative_digit_idx]
# 대표 이미지의 인덱스 사용하여 훈련 세트에서 해당 이미지 추출 -> 변수에 저장

그림 설명
이 그림은 각 클러스터의 대표 이미지를 보여줌 총 50개의 클러스터가 있으며
각 클러스터마다 하나의 대표의 이미지가 선택됨
이러한 대표 이미지는 클러스터의 센트로이드와 가장 가까운 샘플을 나타내며, 각 클러스터의 특징 보여줌
```

### 9.1 (23)
```
- 이미지를 보고 수동으로 레이블 할당
y_representative_digits = np.array([1, 3, 6, 0, 7, 9, 2, ..., 5, 1, 9, 9, 3, 7])
# 대표이미지의 실제 레이블을 배열로 저장

- 레이블된 50개의 샘플로 이루어진 데이터셋의 성능 확인
>>> log_reg = LogisticRegression(max_iter=10_000) # 최대 반복횟수 만번
>>> log_reg.fit(X_representative_digits, y_representative_digits)
# 대표 이미지 샘플과 그 레이블을 사용해 회귀 모델 학습
>>> log_reg.score(X_test, y_test)
0.8488664987405542
# 테스트 세트를 사용하여 로지스틱 회귀 모델의 정확도를 측

- 레이블 전파 label propagation - 레이블과 동일한 클러스터에 있는 모든 샘플로 전파
y_train_propagated = np.empty(len(X_train), dtype=np.int64)
# X_train과 동일한 크기의 빈 배열을 생성하고 데이터 타입 지정 
for i in range(k):
    y_train_propagated[kmeans.labels_ == i] = y_representative_digits[i]
# k개의 클러스터에 대해 반복, 각 클러스터에 속하는 모든 샘플에 대표 이미지의 레이블 할당

요약!!!
1. 대표 이미지 샘플에 수동으로 레이블 할당, 이를 사용해 로지스틱 회귀 모델 학습
2. 모델의 성능 확인
3. 레이블 전파를 통해 클러스터 내 모든 샘플에 동일한 레이블 할당
```


### 9.1 (24)

```
log_reg = LogisticRegression()
log_reg.fit(X_train, y_train_propagated)
log_reg.score(X_test, y_test)
# 전파된 레이블 사용해 로지스틱 회귀 모델 학습
# 학습된 모델을 테스트 세트에서 평가해 정확도 반환
-

- 클러스터에 중심에서 가장 먼 1% 샘플 무시해 일부 이상치 제거 
```


### 9.1 (25)
```
훈련된 모델은 레이블이 전파된 샘플을 사용하여 높은 정확도 달성함
적은 수의 레이블로도 높은 정확도 얻음 

+++
사이킷런은 레이블을 자동으로 전파할 수 있는 두 개의 클래스를 제공
LabelSpreading과 LabelPropagation: 빈지도 학습에서 레이블을 전파하는데 사용
```


### 9.1 (27)
```
- DBSCAN
: 밀집된 연속적 지역을 클러스터로 정의하는 알고리즘

- 작동방식
1. 이웃지역
알고리즘이 각 샘플에서 작은 거리인 엡실론 내에 샘플이 몇 개 놓여있는지 계산
이 지역 내에 있는 샘플을 엡실론 이웃이라고 함

2. 핵심 샘플
각 샘플이 자신을 포함하여 엡실론 이웃에 적어도 min_samples이 있다면 이를 핵심 샘플이라고 간주
핵심 샘플은 밀집된 지역에 있는 샘플

3. 클러스터 형성
핵심 샘플의 이웃에 있는 모든 샘플은 동일한 클러스터
이웃에는 다른 핵심 샘플이 포함될 수 있음 핵심샘플의 이웃은 계속해서 하나의 클러스터 형성

4. 이상치
핵심 샘플도 아니고 이웃도 아닌 샘플을 이상치로 판단

주요 특징
DBSCAN은 클러스터의 형태와 크기와 관계없이 밀집된 지역을 클러스터로 정의
클러스터의 개수를 미리 지정할 필요가 없음
밀도가 낮은 지역, 이상치를 효과적으로 감지가능함
잡음이 많은 데이터에 유용하게 사용
```

### 9.1 (28)
```
- DBSCAN은 모든 클러스터가 밀집되지 않은 지역과 잘 구분될 때 좋은 성능을 발휘
밀집된 지역을 클러스터로 정의하고 밀도가 낮은 영역을 이상치로 간주

엡실론: 두 샘플이 서로의 이웃인지 결정하는 최대 거리
min_samples: 한 샘플이 핵심 샘플로 간주됮기 위해 필요한 최소 이웃 샘플 수

from sklearn.cluster import DBSCAN
from sklearn.datasets import make_moons
X, y = make_moons(n_samples=1000, noise=0.05)
# 천 개의 샘플과 약간의 잡음 가진 두 개의 반달 모양 데이터셋 생성
dbscan = DBSCAN(eps=0.05, min_samples=5)
dbscan.fit(X)
# 엡실론과 최소 이웃 샘플 수를 지정하고 DBSCAN 모델 생성, 데이터를 학습

dbscan.labels_
# 레이블 저장 및 이상치 확인, 모든 샘플의 레이을 labels_ 속성에 저장
일부 샘플의 인덱스는 -1인데 이는 이 샘플을 이상치로 판단했다는 것 


```


### 9.1 (29)

```
핵심 샘플은: DBSCAN에서 밀집된 지역을 클러스터로 정의할 때 기준이 되는 샘플들
엡실론 거리 내에 최소 샘플 이상의 이웃이 있는 샘플

len(dbscan.core_sample_indices_)
# 핵심 샘플의 인덱스 배열 길이 확인 총 808개의 핵심 샘플 있음

- 핵심 샘플의 인덱스는 인스턴스 변수인 core_sample_indices_에 저장

- 핵심 샘플의 실제 데이터 값은 components_ 속성에 저

```
### 9.1 (30)
```
이웃샘플: 핵심 샘플 주의의 샘플로, 핵심 샘플과 가까움
이상치: 핵심이나 이웃 샘플이 아님

왼쪽 그림: 클러스터 7개이고 많은 샘플을 이상치로 판단함
작은 엡실론 값으로 인해 매우 밀집된 작은 클러스터들이 형성, 클러스터의 경계에서 많은 샘플이 이상치 처리

오른쪽 그림: 엡실론 값을 증가시키면 샘플들이 더 넓은 이웃 범위에 포함, 이상치 수 줄어듦
큰 엡실론 값으로 두 개의 큰 클러스터 형성, 클러스터 경계가 더 명확해
```
![image](https://github.com/simsoohyeon/Machine-Learning-Deep-Learning-Study/assets/127268889/25cad5ab-ac7a-4b24-9ad8-b35786b9a684)

### 9.1 (31)
```
- DBSCAN 클래스는 predict() 메소드 제공하지 않고 fit_predcit() 메소드 제공
-> 새로운 샘플에 대해 클러스터 예측이 아닌 기존의 데이터에 대해 클러스터 형성

KNeighborsClassifier: 새로운 데이터 포인트가 어떤 클래스에 속할 가능성이 높은지 예측
DBSCAN의 핵심 샘플과 레이블을 사용하여 학습시킨 뒤 새로운 데이터에 대해 클러스터 예측 
  
```
### 9.1 (32)
```
그림은 두 클러스터 사이의 결정 경계를 보여줌
이 그림은 DBSCAN 알고리즘을 사용하여 클러스터링을 수행한 후,
KNeighborsClassifier를 사용하여 새로운 데이터 포인트 클러스터를 에측함

결정 경계: 두 클러스터 사이의 경계로 새로운 데이터가 어디 속할지 결정하는 기준
```
![image](https://github.com/simsoohyeon/Machine-Learning-Deep-Learning-Study/assets/127268889/a7cf4d5f-e8b8-4199-8c08-455143afaa34)


### 9.1 (33)
```
- 다른 군집 알고리즘

1. 병합 군집: 클러스터의 계층을 밑바닥부터 위로 쌓아 구성
2. BIRCH: 대규모 데이터셋을 위해 고안
3. 평균-이동: 각 샘플을 중심으로 하는 원을 그리고 원마다 안에 포함된 모든 샘플의 평균 구함
4. 유사도 전파: 모든 샘플은 자신을 대표할 다른 샘플을 선택할 때까지 반복적으로 메시지 교환
5. 스펙트럼 군집: 샘플 사이의 유사도 행렬을 받아 임베딩을 만듦, 행렬의 차원 축
```

### 9.2 가우스 혼합 (1)
```
- 가우스 혼합 모델
샘플이 파라미터가 알려지지 않은 여러 개의 혼합된 가우스 분포에서 생성되었다고 가정하는 확률 모델
이 가정하에 데이터의 분포를 모델링하고 데이터의 클러스터를 식별하고 각 클러스터에 대한 통계적 특성 추정

```


### 9.2 (2)

```
기대화-최대화 (EM) 알고리즘
기댓값 단계 Expectation step
: 각 데이터 포인트가 각 클러스터에 속할 확률, 책임도를 계산
최대화 단계 Maximization step
: 책임도를 사용하여 클러스터의 파라미터, 평균을 업데이트

클러스터에 속할 확률로 샘플에 가중치가 적용
이 확률을 샘플에 대한 클러스터의 책임이라 함
```

### 9.2 (3)

```

```

### 9.2 (4)



### 9.2 (5)


![image](https://github.com/simsoohyeon/Machine-Learning-Deep-Learning-Study/assets/127268889/4c44e9f5-15c1-4abf-a30d-0ab93de50906)

### 9.2 (6)



### 9.2 (7)


![image](https://github.com/simsoohyeon/Machine-Learning-Deep-Learning-Study/assets/127268889/2b7458b7-4866-4f12-91b4-f37e85b62bba)
### 9.2 (8)


![image](https://github.com/simsoohyeon/Machine-Learning-Deep-Learning-Study/assets/127268889/618d1fa3-9b8b-47e5-8e68-2da53bba3063)

### 9.2 (13)
